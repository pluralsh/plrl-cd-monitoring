namespaceOverride: {{ configuration.namespace }}
prometheus:
  prometheusSpec:
    prometheusExternalLabelName: {{ cluster.Handle }}
    # incoming metrics from workload clusters will have a cluster label set to the cluster handle, but that's only assigned at the remote write push
    # the mgmt cluster itself will not have a cluster label set, one can use an external label, but that's only added on push time
    # ideally we would have a scrape class that would add a cluster label to all targets scraped by prometheus, this is currently not supported, but will be with probably the next release of the prometheus operator
    # in the meantime we add relabel config to all servicemonitors individually (see below)
    # this is how it would look like:
    # use scrape classes in the future to add a cluster label to all targets scraped by prometheus
    # this will make sure we can always identify the cluster a target belongs to, even for the mgmt cluster prometheus
    # https://github.com/prometheus-operator/prometheus-operator/pull/5978
    # https://github.com/prometheus-operator/prometheus-operator/pull/6379
    #additionalConfig:
    #  scrapeClasses:
    #    - name: cluster_label
    #      default: true
    #      relabelings:
    #        - sourceLabels: []
    #          targetLabel: cluster
    #          replacement: {{ cluster.Handle }}
  serviceMonitor:
    enabled: true
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}

alertmanager:
  serviceMonitor:
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}

grafana:
  serviceMonitor:
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}

kubeApiServer:
  serviceMonitor:
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}


kubelet:
  serviceMonitor:
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}


coreDns:
  serviceMonitor:
    enabled: true
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}

# already monitored with coreDns
kubeDns:
  serviceMonitor:
    enabled: true
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}

kubeProxy:
  enabled: true
  serviceMonitor:
    enabled: true
    relabelings:
      - sourceLabels: []
        targetLabel: cluster
        replacement: {{ cluster.Handle }}

kubeStateMetrics:
  enabled: true

kube-state-metrics:
  fullnameOverride: kps-kube-state-metrics
  selfMonitor:
    enabled: true
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - sourceLabels: []
          targetLabel: cluster
          replacement: {{ cluster.Handle }}

nodeExporter:
  enabled: true

prometheus-node-exporter:
  fullnameOverride: kps-node-exporter
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - sourceLabels: []
          targetLabel: cluster
          replacement: {{ cluster.Handle }}

  resources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2048Mi

# EKS hides metrics for controller manager, scheduler, and etcd
# https://github.com/aws/eks-anywhere/issues/4405
# disable kube controller manager scraping
kubeControllerManager:
  enabled: false

# disable kube scheduler scraping
kubeScheduler:
  enabled: false

kubeEtcd:
  enabled: false
